---
title: "First Steps"
---

Now that you have installed R and RStudio we can move onto starting a project, installing packages and writing some R
code.


## Installing Packages

The core of R is fully functional, you can do a lot with it, but the beauty and power of most languages is that they are
extensible and people can write additional packages that add functionality. R is no exception, there are over 20000
packages available on the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org/).

There is a suite/group of packages collectively called the [Tidyverse](https://www.tidyverse.org/) that follow a
consistent design philosophy and grammar that make writing and using R much easier. These are worth using but need
installing and are explained well in the _R for Data Science (2e)_ book detailed below.

You can easily install them from the _Console_ with.

```{r}
#| echo: true
#| eval: true
#| output: true
install.packages("tidyverse")
install.packages("Hmisc")
## All in one
install.packages(c("tidyverse", "Hmisc"))
```

Once installed you need to load the packages into R and so in RStudio you would type in the Console

```{r}
#| echo: true
#| eval: true
#| output: true
library(tidyverse)
library(Hmisc)
```

All the packages and programmes are now available to use, this means you can use the functions directly without
specifying the package from which they come, for example `filter()`, `select()` or `describe()`. However for clarity in
this document I have stuck with the convention of specifying the package to which a function belongs to make things a
little clearer. This is done by preceding the function with the package name and separating with two colons,
i.e. `dplyr::filter()`, `dplyr::select()` or `Hmisc::describe()`. One of the advantages of this convention is that you
don't _have_ to load the package with the `library()` command.

## Writing R Code

You can write R commands directly into the _Console_ of RStudio and execute them, this is easy and convenient and R will
keep a history of the commands you type that you can go back and forth through using the up and down arrows (it writes
them to the '.Rhistory' file in the project directory root). However, long term you want to have a more structured
record of the code you write which is reproducible so its better to write your commands in a file and execute them from
there.

One of the panels in RStudio has a tab labelled _Files_ and from here you can create new folders and files. Using the
_New Folder_ button create a director called `r`, double-click on it to enter that directory and then use the _New Blank
File_ and select _R Script_ to create a file called _my_first_script.R_.  This opens up the file and you can start
typing in it.

## Reading data.


The file [`example.csv`](data/csv/example.csv) includes some dummy data I made up to demonstrate how
to de-deuplicate data. Right-click and save this file to `getting-started-with-r/data/csv` where you are going to try
things out (you will need to create the nested `data/csv` directory within wherever you created `getting-started-with-r`
in the previous step).

Reading the CSV into memory is done with the `read_csv()` command, and you assign the data to a named object, in this
case `df` (short for `d`ata `f`rame but it could be `dog` or `cat` it doesn't matter) using the `<-` assignment operator.

```{r}
#| echo: true
#| eval: true
#| output: true
df <- read_csv("data/csv/example.csv")
```

This tells us that the data has been read and there are five rows in the data and four columns. The names of the columns
are listed with their types

**NB** If you _didn't_ first run `library(tidyverse)` you will see an error message saying `could not find function
read_csv` because the `read_csv()` function is part of the `tidyverse` library.

This loads the data into an "object" called `df` and we can look at the data by simply typing its name...

```{r}
#| echo: true
#| eval: true
#| output: true
df
```

This is a small dataset so we're able to look at it and make sense of it but often datasets are a lot larger and
"looking" at data isn't that useful. What is more useful is to _describe_, _summarise_ and _visualise_ the data.

If we want to look at just one column there are a couple of ways of doing so, we can use the `$` operator on the data
frame to select the column...

```{r}
#| echo: true
#| eval: true
#| output: true
df$name
```

There are however different ways of selecting and filtering data which are perhaps a little more intuitive and certainly
more flexible.

## Filtering and Selecting Data

Being able to filter and select data is really useful, it narrows down the variables that are included and can make
using other functions considerably easier.  `dplyr` has two verbs for doing this, we have already seen the `select()`
variable that allows us to choose columns, the other verb is `filter()` which allows us to select rows.

### `dplyr::select()`

We can use [dplyr::select()](https://dplyr.tidyverse.org/reference/select.html) to select columns that we want to
keep/see. Here we use the built-in pipe operator `|>` which passes what is on the left (`df`) into what is on the right
(the function `dplyr::select()`). There are other pipe operators available from the [magrittr
package](https://magrittr.tidyverse.org/).

```{r}
#| echo: true
#| eval: true
#| output: true
df |> dplyr::select(name)
```

### `dplyr::filter()`

The `dplyr::filter()` command allows you to select rows that you wish to subset data by. For example if we wanted to
select all `cats` from our example data we would filter based on the `species` column.

```{r}
#| echo: true
#| eval: true
#| output: true
df |>
    dplyr::filter(species == "cat")

```

We can combine `dplyr::filter()` and `dplyr::select()` to filter on rows and columns. You may want to filter your data
first as some of the columns you are filtering on you may not want to retain. In this example we filter on `species` but
then only select the `name` and `age` columns.

```{r}
#| echo: true
#| eval: true
#| output: true
df |>
    dplyr::filter(species == "cat") |>
    dplyr::select(name, age)

```

Or


## Describing Data

Describing data in programming often involves simply saying what the data is. Data tables in R can be thought of as
analogous to worksheets within a spreadsheet and so there are columns which define variables and rows which define
observations/entries.  Each column has a unique data "type" and R does its best to guess what this should be and apply
it.

Usefully, the `read_csv()` function gives us a description of the data we have loaded for free when reading the files,
but you may have derived datasets or just want to check the types of data in your data frame after making changes. We
can do this using the `describe()` functional

```{r}
#| echo: true
#| eval: true
#| output: true
df |> Hmisc::describe()
```

This gives us a nice simple summary of the data with the number of missing and distinct values for all variables,
frequency and proportions for categorical string variables (`name`, `gender`, `species`) and for the numerical variable
`age` some simple summary statistics.

We can also make simple tables of data by selecting columns and then pipping them into the `table()`


```{r}
#| echo: true
#| eval: true
#| output: true
df |> dplyr::select(name) |> table()
```

By default the `table()` command doesn't show any values that are missing which R represents internally as `NA`, you
have to explicitly display these using the `useNA` option.


```{r}
#| echo: true
#| eval: true
#| output: true
df |> dplyr::select(name) |> table(useNA = "ifany")
```

It makes no difference here because there aren't any missing names, but if we apply it to age it does make a difference
since `fluff` is missing his age.

```{r}
#| echo: true
#| eval: true
#| output: true
df |> dplyr::select(age) |> table(useNA = "ifany")
```

## Summarising Data

We can easily summarise data using the [`dplyr::summarise()`](https://dplyr.tidyverse.org/reference/summarise.html) (or `summarize()`) function from `dplyr`.

```{r}
#| echo: true
#| eval: true
#| output: true
df |> dplyr::summarise(mean = mean(age),
                       sd = sd(age),
                       n = n())
```

## Grouping Data

Often you want to perform summaries on groups of data. `dplyr` makes this straight-forward as it has the verb
`groupby()` which groups the data and subsequent commands are performed on each group. Here we group by `species` and

## Help Pages

How do we know what options a function has? Well each package and function has excellent built-in help we can access at
the command line. All you need to do is precede the function you want to look at the help for with a `?` and the help
will be shown.

```{r}
#| echo: true
#| eval: false
?table
table                   package:base                   R Documentation

Cross Tabulation and Table Creation

Description:

     ‘table’ uses cross-classifying factors to build a contingency
     table of the counts at each combination of factor levels.

Usage:

     table(...,
           exclude = if (useNA == "no") c(NA, NaN),
           useNA = c("no", "ifany", "always"),
           dnn = list.names(...), deparse.level = 1)

     as.table(x, ...)
     is.table(x)

     ## S3 method for class 'table'
     as.data.frame(x, row.names = NULL, ...,
                   responseName = "Freq", stringsAsFactors = TRUE,
                   sep = "", base = list(LETTERS))

Arguments:

...
```

Try looking at the help for `dplyr::select()`
